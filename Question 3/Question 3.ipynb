{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install GPUtil\n",
        "import zipfile\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import gc\n",
        "import random\n",
        "import math\n",
        "from GPUtil import showUtilization as gpu_usage\n",
        "from numba import cuda\n",
        "\n",
        "\n",
        "zip_file_path = '/content/aksharantar_sampled.zip'\n",
        "extracted_folder_path = '/content/'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "extracted_folder_contents = os.listdir(extracted_folder_path)\n",
        "print(\"Contents of extracted folder:\", extracted_folder_contents)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(\"Trained on: \" + str(device))\n",
        "\n",
        "train_dataset = pd.read_csv('/content/aksharantar_sampled/hin/hin_train.csv', names=['English', 'Hindi'], header=None)\n",
        "test_dataset = pd.read_csv('/content/aksharantar_sampled/hin/hin_test.csv', names=['English', 'Hindi'], header=None)\n",
        "val_dataset = pd.read_csv('/content/aksharantar_sampled/hin/hin_valid.csv', names=['English', 'Hindi'], header=None)\n",
        "\n",
        "\n",
        "def clear_gpu_cache():\n",
        "    print(\"Initial GPU Usage\")\n",
        "    gpu_usage()\n",
        "    torch.cuda.empty_cache()\n",
        "    cuda.select_device(0)\n",
        "    cuda.close()\n",
        "    cuda.select_device(0)\n",
        "    print(\"GPU Usage after emptying the cache\")\n",
        "    gpu_usage()\n",
        "\n",
        "def split_into_tokens(word):\n",
        "    tokens = []\n",
        "    for x in word:\n",
        "        tokens.append(x)\n",
        "    return tokens\n",
        "\n",
        "def encode_english(word):\n",
        "    tokens = []\n",
        "    for x in word:\n",
        "        tokens.append(eng_dict[x])\n",
        "    for x in range(len(tokens), max_english_length):\n",
        "        tokens.append(eng_dict['<pad>'])\n",
        "    return tokens\n",
        "\n",
        "def encode_hindi(word):\n",
        "    tokens = []\n",
        "    for x in word:\n",
        "        tokens.append(hin_dict[x])\n",
        "    tokens.append(hin_dict['<eow>'])\n",
        "    for x in range(len(tokens), max_hindi_length + 1):\n",
        "        tokens.append(hin_dict['<pad>'])\n",
        "    return tokens\n",
        "\n",
        "def encode_test_english(word):\n",
        "    tokens = []\n",
        "    for x in word:\n",
        "        tokens.append(eng_dict[x])\n",
        "    for x in range(len(tokens), test_max_english_length):\n",
        "        tokens.append(eng_dict['<pad>'])\n",
        "    return tokens\n",
        "\n",
        "def encode_test_hindi(word):\n",
        "    tokens = []\n",
        "    for x in word:\n",
        "        tokens.append(hin_dict[x])\n",
        "    tokens.append(hin_dict['<eow>'])\n",
        "    for x in range(len(tokens), test_max_hindi_length):\n",
        "        tokens.append(hin_dict['<pad>'])\n",
        "    return tokens\n",
        "\n",
        "def encode_val_english(word):\n",
        "    tokens = []\n",
        "    for x in word:\n",
        "        tokens.append(eng_dict[x])\n",
        "    for x in range(len(tokens), val_max_english_length):\n",
        "        tokens.append(eng_dict['<pad>'])\n",
        "    return tokens\n",
        "\n",
        "def encode_val_hindi(word):\n",
        "    tokens = []\n",
        "    for x in word:\n",
        "        tokens.append(hin_dict[x])\n",
        "    tokens.append(hin_dict['<eow>'])\n",
        "    for x in range(len(tokens), val_max_hindi_length):\n",
        "        tokens.append(hin_dict['<pad>'])\n",
        "    return tokens\n",
        "\n",
        "def get_word(characters):\n",
        "    return \"\".join(characters)\n",
        "\n",
        "def calculate_accuracy(target, predictions, flag):\n",
        "    total = 0\n",
        "    for x in range(len(target)):\n",
        "        if torch.equal(target[x], predictions[x]):\n",
        "            total += 1\n",
        "    return total\n",
        "\n",
        "def translate_predictions(target, predictions, df):\n",
        "    i = len(df)\n",
        "    for x in range(len(predictions)):\n",
        "        original = []\n",
        "        for y in target[x]:\n",
        "            if y != 1:\n",
        "                original.append(y)\n",
        "            else:\n",
        "                break\n",
        "        predicted = []\n",
        "        for y in predictions[x]:\n",
        "            if y != 1:\n",
        "                predicted.append(y)\n",
        "            else:\n",
        "                break\n",
        "        df.loc[i, ['Original']] = get_word([reverse_hin[x.item()] for x in original])\n",
        "        df.loc[i, ['Predicted']] = get_word([reverse_hin[x.item()] for x in predicted])\n",
        "        i += 1\n",
        "    return df\n",
        "\n",
        "split_into_tokens(train_dataset.iloc[0]['Hindi'])\n",
        "\n",
        "max_english_length = 0\n",
        "max_hindi_length = 0\n",
        "test_max_english_length = 0\n",
        "test_max_hindi_length = 0\n",
        "\n",
        "for x in range(len(test_dataset)):\n",
        "    temp = 0\n",
        "    for y in test_dataset.iloc[x]['English']:\n",
        "        temp += 1\n",
        "    test_max_english_length = max(test_max_english_length, temp)\n",
        "\n",
        "for x in range(len(test_dataset)):\n",
        "    temp = 0\n",
        "    for y in test_dataset.iloc[x]['Hindi']:\n",
        "        temp += 1\n",
        "    test_max_hindi_length = max(test_max_hindi_length, temp)\n",
        "\n",
        "val_max_english_length = 0\n",
        "val_max_hindi_length = 0\n",
        "\n",
        "for x in range(len(val_dataset)):\n",
        "    temp = 0\n",
        "    for y in val_dataset.iloc[x]['English']:\n",
        "        temp += 1\n",
        "    val_max_english_length = max(val_max_english_length, temp)\n",
        "\n",
        "for x in range(len(val_dataset)):\n",
        "    temp = 0\n",
        "    for y in val_dataset.iloc[x]['Hindi']:\n",
        "        temp += 1\n",
        "    val_max_hindi_length = max(val_max_hindi_length, temp)\n",
        "\n",
        "english_vocab = []\n",
        "for x in range(len(train_dataset)):\n",
        "    temp = 0\n",
        "    for y in train_dataset.iloc[x]['English']:\n",
        "        temp += 1\n",
        "        if y not in english_vocab:\n",
        "            english_vocab.append(y)\n",
        "    if temp > max_english_length:\n",
        "        max_english_length = max(max_english_length, temp)\n",
        "\n",
        "hindi_vocab = []\n",
        "for x in range(len(train_dataset)):\n",
        "    temp = 0\n",
        "    for y in train_dataset.iloc[x]['Hindi']:\n",
        "        temp += 1\n",
        "        if y not in hindi_vocab:\n",
        "            hindi_vocab.append(y)\n",
        "    max_hindi_length = max(temp, max_hindi_length)\n",
        "for x in range(len(test_dataset)):\n",
        "    for y in test_dataset.iloc[x]['Hindi']:\n",
        "        if y not in hindi_vocab:\n",
        "            hindi_vocab.append(y)\n",
        "\n",
        "english_vocab = sorted(english_vocab)\n",
        "hindi_vocab = sorted(hindi_vocab)\n",
        "\n",
        "eng_dict = {}\n",
        "reverse_eng = {}\n",
        "\n",
        "for x in range(len(english_vocab)):\n",
        "    eng_dict[english_vocab[x]] = x + 3\n",
        "    reverse_eng[x + 3] = english_vocab[x]\n",
        "eng_dict['<sow>'] = 0\n",
        "eng_dict['<eow>'] = 1\n",
        "eng_dict['<pad>'] = 2\n",
        "reverse_eng[0] = '<sow>'\n",
        "reverse_eng[1] = '<eow>'\n",
        "reverse_eng[2] = '<pad>'\n",
        "\n",
        "hin_dict = {}\n",
        "reverse_hin = {}\n",
        "for x in range(len(hindi_vocab)):\n",
        "    hin_dict[hindi_vocab[x]] = x + 3\n",
        "    reverse_hin[x + 3] = hindi_vocab[x]\n",
        "hin_dict['<sow>'] = 0\n",
        "hin_dict['<eow>'] = 1\n",
        "hin_dict['<pad>'] = 2\n",
        "reverse_hin[0] = '<sow>'\n",
        "reverse_hin[1] = '<eow>'\n",
        "reverse_hin[2] = '<pad>'\n",
        "\n",
        "encode_english(train_dataset.iloc[0]['English'])\n",
        "\n",
        "eng_words = []\n",
        "hin_words = []\n",
        "for x in range(len(train_dataset)):\n",
        "    eng_words.append(encode_english(train_dataset.iloc[x]['English']))\n",
        "    hin_words.append(encode_hindi(train_dataset.iloc[x]['Hindi']))\n",
        "eng_words = torch.tensor(eng_words)\n",
        "hin_words = torch.tensor(hin_words)\n",
        "max_hindi_length\n",
        "\n",
        "max_hindi_length += 1\n",
        "test_max_hindi_length += 1\n",
        "val_max_hindi_length += 1\n",
        "max_hindi_length\n",
        "\n",
        "val_eng_words = []\n",
        "val_hin_words = []\n",
        "for x in range(len(val_dataset)):\n",
        "    val_eng_words.append(encode_val_english(val_dataset.iloc[x]['English']))\n",
        "    val_hin_words.append(encode_val_hindi(val_dataset.iloc[x]['Hindi']))\n",
        "val_eng_words = torch.tensor(val_eng_words)\n",
        "val_hin_words = torch.tensor(val_hin_words)\n",
        "\n",
        "test_eng_words = []\n",
        "test_hin_words = []\n",
        "for x in range(len(test_dataset)):\n",
        "    test_eng_words.append(encode_test_english(test_dataset.iloc[x]['English']))\n",
        "    test_hin_words.append(encode_test_hindi(test_dataset.iloc[x]['Hindi']))\n",
        "test_eng_words = torch.tensor(test_eng_words)\n",
        "test_hin_words = torch.tensor(test_hin_words)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, char_embed_size, hidden_size, no_of_layers, dropout, rnn):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layer = no_of_layers\n",
        "        self.rnn = rnn\n",
        "        self.embedding = nn.Embedding(len(eng_dict), char_embed_size).to(device)\n",
        "        self.embedding.weight.requires_grad = True\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.LSTM = nn.LSTM(char_embed_size, hidden_size, self.layer, batch_first=True, bidirectional=True).to(device)\n",
        "        self.RNN = nn.RNN(char_embed_size, hidden_size, self.layer, batch_first=True, bidirectional=True).to(device)\n",
        "        self.GRU = nn.GRU(char_embed_size, hidden_size, self.layer, batch_first=True, bidirectional=True).to(device)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        embedded = self.embedding(input)\n",
        "        embedded1 = self.drop(embedded)\n",
        "        cell1 = cell\n",
        "        if self.rnn == 'RNN':\n",
        "            output, hidden1 = self.RNN(embedded1, hidden)\n",
        "        elif self.rnn == 'LSTM':\n",
        "            output, (hidden1, cell1) = self.LSTM(embedded1, (hidden, cell))\n",
        "        elif self.rnn == 'GRU':\n",
        "            output, hidden1 = self.GRU(embedded1, hidden)\n",
        "        return output, (hidden1, cell1)\n",
        "\n",
        "\n",
        "class DecoderNoAttention(nn.Module):\n",
        "    def __init__(self, char_embed_size, hidden_size, no_of_layers, dropout, batchsize, rnn):\n",
        "        super(DecoderNoAttention, self).__init__()\n",
        "        self.layer = no_of_layers\n",
        "        self.batchsize = batchsize\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = rnn\n",
        "        self.embedding = nn.Embedding(len(hin_dict), char_embed_size).to(device)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.embedding.weight.requires_grad = True\n",
        "        self.LSTM = nn.LSTM(char_embed_size + hidden_size * 2, hidden_size, self.layer, batch_first=True).to(device)\n",
        "        self.RNN = nn.RNN(char_embed_size + hidden_size * 2, hidden_size, self.layer, batch_first=True).to(device)\n",
        "        self.GRU = nn.GRU(char_embed_size + hidden_size * 2, hidden_size, self.layer, batch_first=True).to(device)\n",
        "        self.linear = nn.Linear(hidden_size, len(hin_dict), bias=True).to(device)\n",
        "        self.softmax = nn.Softmax(dim=2).to(device)\n",
        "\n",
        "    def forward(self, input, hidden, cell, og_hidden, matrix):\n",
        "        embedded = self.embedding(input)\n",
        "        s1 = og_hidden.size()[1]\n",
        "        s2 = og_hidden.size()[2]\n",
        "        embedded1 = torch.cat((embedded, og_hidden[0].resize(s1, 1, s2), og_hidden[1].resize(s1, 1, s2)), dim=2)\n",
        "        embedded2 = self.drop(embedded1)\n",
        "        cell1 = cell\n",
        "        if self.rnn == 'LSTM':\n",
        "            output, (hidden1, cell1) = self.LSTM(embedded2, (hidden, cell))\n",
        "        elif self.rnn == 'RNN':\n",
        "            output, hidden1 = self.RNN(embedded2, hidden)\n",
        "        elif self.rnn == 'GRU':\n",
        "            output, hidden1 = self.GRU(embedded2, hidden)\n",
        "        output1 = self.linear(output)\n",
        "        return output1, (hidden1, cell1)\n",
        "\n",
        "def val_evaluate(attention, val_eng_words, val_hin_words, encoder, decoder, batch_size, hidden_size, char_embed_size, no_of_layers):\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for x in range(0, len(val_dataset), batch_size):\n",
        "            loss = 0\n",
        "            input_tensor = val_eng_words[x:x + batch_size].to(device)\n",
        "            if input_tensor.size()[0] < batch_size:\n",
        "                break\n",
        "            en_hidden = torch.zeros(2 * no_of_layers, batch_size, hidden_size).to(device)\n",
        "            en_cell = torch.zeros(2 * no_of_layers, batch_size, hidden_size).to(device)\n",
        "            output, (hidden, cell) = encoder.forward(input_tensor, en_hidden, en_cell)\n",
        "            del input_tensor\n",
        "            del en_hidden\n",
        "            del en_cell\n",
        "            output = torch.split(output, [hidden_size, hidden_size], dim=2)\n",
        "            output = torch.add(output[0], output[1]) / 2\n",
        "            input2 = []\n",
        "            for y in range(batch_size):\n",
        "                input2.append([0])\n",
        "            input2 = torch.tensor(input2).to(device)\n",
        "            hidden = hidden.resize(2, no_of_layers, batch_size, hidden_size)\n",
        "            hidden1 = torch.add(hidden[0], hidden[1]) / 2\n",
        "            cell = cell.resize(2, no_of_layers, batch_size, hidden_size)\n",
        "            cell1 = torch.add(cell[0], cell[1]) / 2\n",
        "            OGhidden = hidden1\n",
        "            predicted = []\n",
        "            predictions = []\n",
        "            if attention:\n",
        "                temp = output\n",
        "            else:\n",
        "                temp = OGhidden\n",
        "            for i in range(val_max_hindi_length):\n",
        "                output1, (hidden1, cell1) = decoder.forward(input2, hidden1, cell1, temp, False)\n",
        "                predicted.append(output1)\n",
        "                output2 = decoder.softmax(output1)\n",
        "                output3 = torch.argmax(output2, dim=2)\n",
        "                predictions.append(output3)\n",
        "                input2 = output3\n",
        "            predicted = torch.cat(tuple(x for x in predicted), dim=1).to(device).resize(val_max_hindi_length * batch_size, len(hin_dict))\n",
        "            predictions = torch.cat(tuple(x for x in predictions), dim=1).to(device)\n",
        "            total_acc += calculate_accuracy(val_hin_words[x:x + batch_size].to(device), predictions, x)\n",
        "            loss = nn.CrossEntropyLoss(reduction='sum')(predicted, val_hin_words[x:x + batch_size].reshape(-1).to(device))\n",
        "            with torch.no_grad():\n",
        "                total_loss += loss.item()\n",
        "        validation_loss = total_loss / (len(val_dataset) * val_max_hindi_length)\n",
        "        validation_accuracy = (total_acc / len(val_dataset)) * 100\n",
        "        del predictions\n",
        "        del predicted\n",
        "        del input2\n",
        "        del output1\n",
        "        del output2\n",
        "        del output3\n",
        "        del hidden1\n",
        "        del cell1\n",
        "        del OGhidden\n",
        "        del output\n",
        "        del cell\n",
        "        return validation_loss, validation_accuracy\n",
        "\n",
        "def test_evaluate(attention, test_eng_words, test_hin_words, encoder, decoder, batch_size, hidden_size, char_embed_size, no_of_layers):\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for x in range(0, len(test_dataset), batch_size):\n",
        "            loss = 0\n",
        "            input_tensor = test_eng_words[x:x + batch_size].to(device)\n",
        "            if input_tensor.size()[0] < batch_size:\n",
        "                break\n",
        "            en_hidden = torch.zeros(2 * no_of_layers, batch_size, hidden_size).to(device)\n",
        "            en_cell = torch.zeros(2 * no_of_layers, batch_size, hidden_size).to(device)\n",
        "            output, (hidden, cell) = encoder.forward(input_tensor, en_hidden, en_cell)\n",
        "            del input_tensor\n",
        "            del en_hidden\n",
        "            del en_cell\n",
        "            output = torch.split(output, [hidden_size, hidden_size], dim=2)\n",
        "            output = torch.add(output[0], output[1]) / 2\n",
        "            input2 = []\n",
        "            for y in range(batch_size):\n",
        "                input2.append([0])\n",
        "            input2 = torch.tensor(input2).to(device)\n",
        "            hidden = hidden.resize(2, no_of_layers, batch_size, hidden_size)\n",
        "            hidden1 = torch.add(hidden[0], hidden[1]) / 2\n",
        "            cell = cell.resize(2, no_of_layers, batch_size, hidden_size)\n",
        "            cell1 = torch.add(cell[0], cell[1]) / 2\n",
        "            OGhidden = hidden1\n",
        "            predicted = []\n",
        "            predictions = []\n",
        "            if attention:\n",
        "                temp = output\n",
        "            else:\n",
        "                temp = OGhidden\n",
        "            for i in range(test_max_hindi_length):\n",
        "                output1, (hidden1, cell1) = decoder.forward(input2, hidden1, cell1, temp, False)\n",
        "                predicted.append(output1)\n",
        "                output2 = decoder.softmax(output1)\n",
        "                output3 = torch.argmax(output2, dim=2)\n",
        "                predictions.append(output3)\n",
        "                input2 = output3\n",
        "            predicted = torch.cat(tuple(x for x in predicted), dim=1).to(device).resize(test_max_hindi_length * batch_size, len(hin_dict))\n",
        "            predictions = torch.cat(tuple(x for x in predictions), dim=1).to(device)\n",
        "            total_acc += calculate_accuracy(test_hin_words[x:x + batch_size].to(device), predictions, x)\n",
        "            loss = nn.CrossEntropyLoss(reduction='sum')(predicted, test_hin_words[x:x + batch_size].reshape(-1).to(device))\n",
        "            with torch.no_grad():\n",
        "                total_loss += loss.item()\n",
        "        test_loss = total_loss / (len(test_dataset) * test_max_hindi_length)\n",
        "        test_accuracy = (total_acc / len(test_dataset)) * 100\n",
        "        del predictions\n",
        "        del predicted\n",
        "        del input2\n",
        "        del output1\n",
        "        del output2\n",
        "        del output3\n",
        "        del hidden1\n",
        "        del cell1\n",
        "        del OGhidden\n",
        "        del output\n",
        "        del cell\n",
        "        return test_loss, test_accuracy\n",
        "\n",
        "def train(batch_size, hidden_size, char_embed_size, no_of_layers, dropout, epochs, rnn):\n",
        "    gc.collect()\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    encoder = Encoder(char_embed_size, hidden_size, no_of_layers, dropout, rnn).to(device)\n",
        "    decoder = DecoderNoAttention(char_embed_size, hidden_size, no_of_layers, dropout, batch_size, rnn).to(device)\n",
        "    # print(encoder.parameters)\n",
        "    # print(decoder.parameters)\n",
        "    opt_encoder = optim.Adam(encoder.parameters(), lr=0.001)\n",
        "    opt_decoder = optim.Adam(decoder.parameters(), lr=0.001)\n",
        "    teacher_ratio = 0.5\n",
        "    epoch_count = 0\n",
        "    for _ in range(epochs):\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for x in range(0, len(train_dataset), batch_size):\n",
        "            loss = 0\n",
        "            opt_encoder.zero_grad()\n",
        "            opt_decoder.zero_grad()\n",
        "            input_tensor = eng_words[x:x + batch_size].to(device)\n",
        "            # taking initial hidden and cell states as (2* no_of_layers, hidden_size, hidden_size) because I have considered encoder to be bidirectional\n",
        "            en_hidden = torch.zeros(2 * no_of_layers, batch_size, hidden_size).to(device)\n",
        "            en_cell = torch.zeros(2 * no_of_layers, batch_size, hidden_size).to(device)\n",
        "            if input_tensor.size()[0] < batch_size:\n",
        "                break\n",
        "            output, (hidden, cell) = encoder.forward(input_tensor, en_hidden, en_cell)\n",
        "            del en_hidden\n",
        "            del en_cell\n",
        "            del input_tensor\n",
        "            input2 = []\n",
        "            for y in range(batch_size):\n",
        "                input2.append([0])\n",
        "            input2 = torch.tensor(input2).to(device)\n",
        "            hidden = hidden.resize(2, no_of_layers, batch_size, hidden_size)\n",
        "            cell = cell.resize(2, no_of_layers, batch_size, hidden_size)\n",
        "            # averaging due to bidirectional encoder\n",
        "            hidden1 = torch.add(hidden[0], hidden[1]) / 2\n",
        "            cell1 = torch.add(cell[0], cell[1]) / 2\n",
        "            OGhidden = hidden1\n",
        "            predicted = []\n",
        "            predictions = []\n",
        "            use_teacher_forcing = True if random.random() < teacher_ratio else False\n",
        "            if use_teacher_forcing:\n",
        "                for i in range(max_hindi_length):\n",
        "                    output1, (hidden1, cell1) = decoder.forward(input2, hidden1, cell1, OGhidden, False)\n",
        "                    predicted.append(output1)\n",
        "                    output2 = decoder.softmax(output1)\n",
        "                    output3 = torch.argmax(output2, dim=2)\n",
        "                    predictions.append(output3)\n",
        "                    input2 = hin_words[x:x + batch_size, i].to(device).resize(batch_size, 1)\n",
        "            else:\n",
        "                for i in range(max_hindi_length):\n",
        "                    output1, (hidden1, cell1) = decoder.forward(input2, hidden1, cell1, OGhidden, False)\n",
        "                    predicted.append(output1)\n",
        "                    output2 = decoder.softmax(output1)\n",
        "                    output3 = torch.argmax(output2, dim=2)\n",
        "                    predictions.append(output3)\n",
        "                    input2 = output3\n",
        "            predicted = torch.cat(tuple(x for x in predicted), dim=1).to(device).resize(max_hindi_length * batch_size, len(hin_dict))\n",
        "            predictions = torch.cat(tuple(x for x in predictions), dim=1).to(device)\n",
        "            total_acc += calculate_accuracy(hin_words[x:x + batch_size].to(device), predictions, x)\n",
        "            loss = nn.CrossEntropyLoss(reduction='sum')(predicted, hin_words[x:x + batch_size].reshape(-1).to(device))\n",
        "            with torch.no_grad():\n",
        "                total_loss += loss.item()\n",
        "            loss.backward(retain_graph=True)\n",
        "            torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=1)\n",
        "            torch.nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=1)\n",
        "            opt_encoder.step()\n",
        "            opt_decoder.step()\n",
        "        del predictions\n",
        "        del predicted\n",
        "        del input2\n",
        "        del output1\n",
        "        del output2\n",
        "        del output3\n",
        "        del hidden1\n",
        "        del cell1\n",
        "        del OGhidden\n",
        "        del output\n",
        "        del cell\n",
        "        training_loss = total_loss / (51200 * max_hindi_length)\n",
        "        training_accuracy = total_acc / 512\n",
        "        validation_loss, validation_accuracy = val_evaluate(False, val_eng_words, val_hin_words, encoder, decoder, batch_size, hidden_size, char_embed_size, no_of_layers)\n",
        "        print(\"Epoch: \" + str(epoch_count + 1) + \"/\" + str(epochs) + \"; Train loss: \" + str(training_loss) + \"; Val loss: \" + str(validation_loss))\n",
        "        epoch_count += 1\n",
        "    return encoder, decoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d_q4DRFEG5H",
        "outputId": "ca491f62-4a94-4e15-9aa2-bf07461718fa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting GPUtil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7394 sha256=8fd4241575de6d8581e748b10f5ed4a3c722154f21b0cf312ca49544a1e7f51b\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil\n",
            "Successfully installed GPUtil-1.4.0\n",
            "Contents of extracted folder: ['.config', '__MACOSX', 'aksharantar_sampled.zip', 'aksharantar_sampled', 'sample_data']\n",
            "Trained on: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_evaluate(attention, test_eng_words, test_hin_words, encoder, decoder, batch_size, hidden_size, char_embed_size, no_of_layers):\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for x in range(0, len(test_dataset), batch_size):\n",
        "            loss = 0\n",
        "            input_tensor = test_eng_words[x:x + batch_size].to(device)\n",
        "            if input_tensor.size()[0] < batch_size:\n",
        "                break\n",
        "            en_hidden = torch.zeros(2 * no_of_layers, batch_size, hidden_size).to(device)\n",
        "            en_cell = torch.zeros(2 * no_of_layers, batch_size, hidden_size).to(device)\n",
        "            output, (hidden, cell) = encoder.forward(input_tensor, en_hidden, en_cell)\n",
        "            del input_tensor\n",
        "            del en_hidden\n",
        "            del en_cell\n",
        "            output = torch.split(output, [hidden_size, hidden_size], dim=2)\n",
        "            output = torch.add(output[0], output[1]) / 2\n",
        "            input2 = []\n",
        "            for y in range(batch_size):\n",
        "                input2.append([0])\n",
        "            input2 = torch.tensor(input2).to(device)\n",
        "            hidden = hidden.resize(2, no_of_layers, batch_size, hidden_size)\n",
        "            hidden1 = torch.add(hidden[0], hidden[1]) / 2\n",
        "            cell = cell.resize(2, no_of_layers, batch_size, hidden_size)\n",
        "            cell1 = torch.add(cell[0], cell[1]) / 2\n",
        "            OGhidden = hidden1\n",
        "            predicted = []\n",
        "            predictions = []\n",
        "            if attention:\n",
        "                temp = output\n",
        "            else:\n",
        "                temp = OGhidden\n",
        "            for i in range(test_max_hindi_length):\n",
        "                output1, (hidden1, cell1) = decoder.forward(input2, hidden1, cell1, temp, False)\n",
        "                predicted.append(output1)\n",
        "                output2 = decoder.softmax(output1)\n",
        "                output3 = torch.argmax(output2, dim=2)\n",
        "                predictions.append(output3)\n",
        "                input2 = output3\n",
        "            predicted = torch.cat(tuple(x for x in predicted), dim=1).to(device).resize(test_max_hindi_length * batch_size, len(hin_dict))\n",
        "            predictions = torch.cat(tuple(x for x in predictions), dim=1).to(device)\n",
        "            total_acc += calculate_accuracy(test_hin_words[x:x + batch_size].to(device), predictions, x)\n",
        "            loss = nn.CrossEntropyLoss(reduction='sum')(predicted, test_hin_words[x:x + batch_size].reshape(-1).to(device))\n",
        "            with torch.no_grad():\n",
        "                total_loss += loss.item()\n",
        "        test_loss = total_loss / (len(test_dataset) * test_max_hindi_length)\n",
        "        test_accuracy = (total_acc / len(test_dataset)) * 100\n",
        "        del predictions\n",
        "        del predicted\n",
        "        del input2\n",
        "        del output1\n",
        "        del output2\n",
        "        del output3\n",
        "        del hidden1\n",
        "        del cell1\n",
        "        del OGhidden\n",
        "        del output\n",
        "        del cell\n",
        "        return test_loss, test_accuracy"
      ],
      "metadata": {
        "id": "zM0xx3iAFY6j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Best config:\n",
        "hidden_size = 256\n",
        "char_embed_size = 256\n",
        "no_of_layers = 2\n",
        "dropout = 0.3\n",
        "rnn = 'LSTM'\n",
        "epochs = 10\n",
        "batchsize = 64\n",
        "Encoder1,Decoder1 = train(batchsize,hidden_size,char_embed_size,no_of_layers,dropout,epochs,rnn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du0O1aWpE4Vt",
        "outputId": "f19e557f-fc2b-43d4-fe12-0cc5c9dcf484"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:868: UserWarning: non-inplace resize is deprecated\n",
            "  warnings.warn(\"non-inplace resize is deprecated\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10; Train loss: 0.8137863657304218; Val loss: 0.5273517980462029\n",
            "Epoch: 2/10; Train loss: 0.48391853179250444; Val loss: 0.4658766637245814\n",
            "Epoch: 3/10; Train loss: 0.40916739787374223; Val loss: 0.4594887782420431\n",
            "Epoch: 4/10; Train loss: 0.36707675232773734; Val loss: 0.4419056475162506\n",
            "Epoch: 5/10; Train loss: 0.345088411853427; Val loss: 0.43049828246945426\n",
            "Epoch: 6/10; Train loss: 0.3128636997938156; Val loss: 0.42128150732744307\n",
            "Epoch: 7/10; Train loss: 0.298293067018191; Val loss: 0.41164006966920125\n",
            "Epoch: 8/10; Train loss: 0.2721450397656077; Val loss: 0.41789535326617105\n",
            "Epoch: 9/10; Train loss: 0.26046356731227466; Val loss: 0.4220333620905876\n",
            "Epoch: 10/10; Train loss: 0.240728776880673; Val loss: 0.428877470039186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_,test_accuracy = test_evaluate(False, test_eng_words, test_hin_words, Encoder1, Decoder1, batchsize, hidden_size, char_embed_size, no_of_layers)\n",
        "print(\"Test accuracy (in %): \" + str(test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUltBU7FFQbr",
        "outputId": "0d635f1b-83c2-484b-8d99-dfc0526ab600"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy (in %): 33.7890625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h2r3-zZOK78q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}